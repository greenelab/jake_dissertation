## Abstract {.page_break_before}

In the pursuit of better molecular characterization of diverse cancers, collaborative efforts have generated large public datasets, which combine various data types and data sources.
Simultaneously, machine learning has rapidly moved toward models with many parameters that can be trained on broad sets of data, and subsequently fine-tuned to a wide variety of tasks.
Computational oncology sits squarely at the intersection between these advances, but the structure of large cancer datasets is uniquely heterogeneous relative to other fields in which large models have proven successful.
In this dissertation, we first show that the choice of optimizer used to fit models on cancer transcriptomics datasets can have pronounced effects on model selection and model tuning.
We then explore two aspects of heterogeneity inherent to public cancer datasets that affect machine learning modeling choices.
We first show that most -omics types can capture information relevant to cancer function, but when combined there is considerable redundancy and model performance does not generally improve.
Next, we study model generalization across cancer types and model systems and its relation to model selection, finding that cross-validation performance on holdout data is a sufficient selection criterion, and incorporating model sparsity/simplicity does not improve generalization performance.
Generally, our results show that the particularities of large cancer genomics datasets must be taken into account for applications of machine learning to be successful in this space.
These findings suggest hurdles to, but also opportunities for, machine learning models integrating pan-cancer and pan-omics data to make biological and clinical predictions.


